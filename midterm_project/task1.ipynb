{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from task1 import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The core idea behind our algorithm is using a binary search tree, where each node has a boolean condition property and each leaf node has a value property. When making a prediction, if the condition property is true for some given input $\\vec{x}$, then it will traverse down the right subtree. Otherwise, it will traverse down the left subtree. These conditions will always be of the form\n",
    "$$\n",
    "\\vec{x}_f \\leq \\text{value}\n",
    "$$\n",
    "where $\\vec{x}_f$ is one of the features of the sample $\\vec{x}$ and *value* is a threshold value that was determined when building the tree. When a leaf node is reached, we return the value property of that leaf node as our prediction.\n",
    "\n",
    "A few fundamental things to consider are\n",
    "1. How easy will it be to build this tree?\n",
    "2. How likely will the tree be balanced, thus increasing prediction performance?\n",
    "\n",
    "For 1, building the tree will be fairly simple if we use partitioning. We will have to write logic for determining if we should add another node to the tree. We have three conditions under which this will happen.\n",
    "1. The impurity of the dataset for the node is 0.\n",
    "2. The maximum number of leaf nodes have already been created.\n",
    "3. The height of the tree has reached the maximum value.\n",
    "4. The dataset has less than three points.\n",
    "\n",
    "For 2, this ultimately depends on how the given training data. In order to guarantee the tree is balanced, we would have to implement an AVL tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Algorithm Pseudo code\n",
    "\n",
    "```\n",
    "fit(X, y)\n",
    "    // initialize a list to use as a queue\n",
    "    node_queue = []\n",
    "    \n",
    "    // starting at the root node \n",
    "    node = BST.root_node\n",
    "    \n",
    "    // computing the impurity of the root node\n",
    "    sse = y.shape[0] * VAR(y)\n",
    "    \n",
    "    // add the root node to the queue\n",
    "    node_queue.enqueue({\n",
    "        node: node\n",
    "        depth: 0\n",
    "        mask: All rows of X\n",
    "        impurity: sse\n",
    "        feature: 0\n",
    "    })\n",
    "\n",
    "    while node_queue is not empty:\n",
    "        // dequeue a node\n",
    "        node_dict = node_queue.dequeue()\n",
    "        \n",
    "        // leaf_size and max_height also have to not be None\n",
    "        if BST.num_of_leaves >= leaf_size or node_dict.depth == max_height or node_dict.impurity == 0 or X has less than 3 rows\n",
    "            set the node's value property to the mean of y\n",
    "            continue to the next iteration\n",
    "        \n",
    "        find the best sample to create a split\n",
    "\n",
    "        get the impurity of each split\n",
    "\n",
    "        if BST.num_of_leaves + 1 <= leaf_size or leaf_size == None\n",
    "            add a new node as the left child of node_dict.node\n",
    "            enqueue a new node_dict to node_queue\n",
    "\n",
    "        if BST.num_of_leaves + 1 <= leaf_size or leaf_size == None\n",
    "            add a new node as the right child of node_dict.node\n",
    "            enqueue a new node_dict to node_queue\n",
    "        \n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tree Structure and Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As stated earlier, we want to use a binary search tree (BST) where each node either has a condition property, or a value property.\n",
    "- The condition property is used to determine which path we should take when traversing down the tree (making a prediction). This property is only given to nodes that are not leaf nodes.\n",
    "- The value property is the value we return for our prediction. Only leaf nodes will have this property.\n",
    "\n",
    "We also want our BST to track it's height and the number of leaf nodes. Or, and this is a more simple approach, we can traverse the tree to calculate these values each time we need them, which at worst cost $O(n)$ time.\n",
    "\n",
    "What is the exact logic we need to determine if we should or shouldn't add a node for cases 2 and 3?\n",
    "\n",
    "For case 2, once we get the current number of leaf nodes, we can determine if we can or cannot add a node based on the following condition. If the number of leaf nodes is less than the max leaf size, add another node. Otherwise, do not add another node (or stop building the tree). The full condition will look like: if the number of leaf nodes is less than the max leaf size, and the impurity of the data is not 0, then add another node. Otherwise, if the max leaf size has been reached, stop building the tree.\n",
    "\n",
    "For case 3, the solution is to actually track each node's depth in the tree, which is an equivalent, but more useful measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding partitioning\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of using partitioning is to avoid splitting a copy of the entire dataset to each individual node while building the tree.\n",
    "\n",
    "Let $X$ denote our dataset and let $\\vec{x}$ denote a sample in $X$.\n",
    "\n",
    "Assume we've found a sample $\\vec{x}^{(s)} \\in X$ to split the data. This means we want to split the data along at the sample $\\vec{x}^{(s)} along the feature $f$. One way we could do this is to partition the dataset into two separate datasets $X_{l}$ and $X_{r}$.\n",
    "\n",
    "The way we do this is by computing the following. For each sample in $X$, if $\\vec{x}_{f} < \\vec{x}^{(s)}_{f}$ then it goes into $X_{l}$, otherwise, it goes into $X_{r}$. Where this becomes efficient is we can simply store the\n",
    "indices for these samples instead of creating copies of the original dataset. For numpy, the easier way is to create a boolean mask for the rows we want in each partition.\n",
    "\n",
    "The following code is an example of how this works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[13, 16,  0, 16],\n",
       "       [ 9, 10, 12,  5],\n",
       "       [19,  1,  5,  7],\n",
       "       [11,  8,  2,  0]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating an array of random integers\n",
    "X = np.random.default_rng(5).integers(20, size=(4, 4))\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "[False False  True  True]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[19,  1,  5,  7],\n",
       "       [11,  8,  2,  0]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's say we want to partition the data along the 2nd feature\n",
    "# and the sample we are splitting the data at is the second sample\n",
    "f = 1 # second feature\n",
    "split_value = X[1, f] # second sample (second row)\n",
    "mask = X[:, f] < split_value\n",
    "print(split_value)\n",
    "print(mask)\n",
    "X_l = X[mask]\n",
    "X_l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A massive issue with this approach is it doesn't work for multiple splits down the tree. That is, how are we supposed to properly calculate the dataset for the node's children, only using a subset of the original dataset? Our algorithm always uses these boolean masks to determine which rows the node has to use from the original dataset. This means we cannot create a mask using a subset to determine which rows in $X$ each node needs to use.\n",
    "\n",
    "To fix this, consider the following scenario. We have a node $N_0$ which is the root node and uses the original dataset $X$. The left node $N_1$ uses a split of the dataset $X_l$. Now, we want to create another node $N_2$ which must have a split $X_{ll}$ of the dataset $X_l$. The boolean mask for $X_{ll}$ must also be used to ignore all the rows in $X$ that were not present in $X_l$. This means we can define $X_{ll}$ as follows.\n",
    "\n",
    "$$\n",
    "X_{ll} = \\{ \\vec{x} | \\vec{x}_f < \\text{split value and } \\vec{x} \\in X_l  \\}\n",
    "$$\n",
    "\n",
    "This can be computed by simply adding performing the following calculation: `(X[:, f] < split_value) & mask` where `mask` is the boolean mask of node's dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False, False,  True])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example, let's say this time we wanted to split the data of X_l along the last feature\n",
    "# and the split value is 7.\n",
    "# the result only keeps the last row of X, which is what we want.\n",
    "X_ll = (X[:, 3] < 7) & mask\n",
    "X_ll"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determining the best split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To determine the best split, we will simply use a combination of a function that calculates the sum of squares for each possible split of the data and numpy's `argmin()` function.\n",
    "\n",
    "First, we start off with a dataset $X$. Then for each sample $\\vec{x} \\in X$, we split the data into two subsets $X_l$ and $X_r$ using our partitioning method. Then, we compute the sum\n",
    "$$\n",
    "\\sum_{i=1}^{n_{l}}{ (y_{i} - \\bar{y}_{l})^{2} } + \\sum_{j=1}^{n_{r}}{ (y_{j} - \\bar{y}_{r})^{2} }\n",
    "$$\n",
    "and use `argmin()` to get the index of the sample which minimizes this sum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[13 16  0 16]\n",
      " [ 9 10 12  5]\n",
      " [19  1  5  7]\n",
      " [11  8  2  0]]\n",
      "[0.80500292 0.80794079 0.51532556 0.28580138]\n",
      "[[19  1  5  7]\n",
      " [11  8  2  0]]\n",
      "8\n",
      "[[13 16  0 16]]\n"
     ]
    }
   ],
   "source": [
    "# creating a test dataset\n",
    "X = np.random.default_rng(5).integers(20, size=(4, 4))\n",
    "y = np.random.default_rng(5).uniform(size=(4,))\n",
    "print(X)\n",
    "print(y)\n",
    "\n",
    "# split the dataset into two disjoint subsets using X[1, 1] (second feature)\n",
    "left_bool_mask = X[:, 1] < X[1, 1]\n",
    "X_l = X[left_bool_mask]\n",
    "\n",
    "right_bool_mask = X[:, 1] > X[1, 1]\n",
    "X_r = X[right_bool_mask]\n",
    "\n",
    "print(X_l)\n",
    "print(X_l.size)\n",
    "print(X_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.02634067482130236"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compute the sum of squares for each split\n",
    "left_sum = X_l.shape[0] * np.var(y[left_bool_mask])\n",
    "right_sum = X_r.shape[0] * np.var(y[right_bool_mask])\n",
    "left_sum + right_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xavier/miniconda3/lib/python3.12/site-packages/numpy/core/_methods.py:206: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "/home/xavier/miniconda3/lib/python3.12/site-packages/numpy/core/_methods.py:163: RuntimeWarning: invalid value encountered in divide\n",
      "  arrmean = um.true_divide(arrmean, div, out=arrmean,\n",
      "/home/xavier/miniconda3/lib/python3.12/site-packages/numpy/core/_methods.py:198: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "# generalizing\n",
    "\n",
    "def split_data(X, sample_idx, feature_idx):\n",
    "    left_bool_mask = X[:, feature_idx] < X[sample_idx, feature_idx]\n",
    "    right_bool_mask = X[:, feature_idx] > X[sample_idx, feature_idx]\n",
    "    return left_bool_mask, right_bool_mask\n",
    "\n",
    "def sum_of_squares(y):\n",
    "    return y.shape[0] * np.std(y)\n",
    "\n",
    "def get_best_split(X, y, feature_idx):\n",
    "    left_bool_mask, right_bool_mask = split_data(X, 1, feature_idx)\n",
    "    best_sample_idx = 1\n",
    "    # best sum of squares error\n",
    "    best_sse = sum_of_squares(y[left_bool_mask]) + sum_of_squares(y[right_bool_mask])\n",
    "    for sample_idx in range(0, X.shape[0] - 1):\n",
    "        left_bool_mask, right_bool_mask = split_data(X, sample_idx, feature_idx)\n",
    "        sse = sum_of_squares(y[left_bool_mask]) + \\\n",
    "              sum_of_squares(y[right_bool_mask])\n",
    "        \n",
    "        if sse < best_sse:\n",
    "            best_sample_idx = sample_idx\n",
    "            best_sse = sse\n",
    "    \n",
    "    return best_sample_idx\n",
    "        \n",
    "\n",
    "print(get_best_split(X, y, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Edge cases**\n",
    "Sometimes, we don't want to compute a split. This happens in three the cases mentioned in *Our Algorithm*.\n",
    "\n",
    "To handle the first case, we should modify `get_best_split()` to also return the best impurity measure. Then, the algorithm will check if it is zero, and will not add any other nodes to the queue.\n",
    "\n",
    "To handle cases 2 and 4, we can create a function which checks the state of the tree (height and number of leaves), and then returns a boolean value to indicate if we can afford to add another node to the tree.\n",
    "\n",
    "To handle case 4, we can simply check the size of the dataset before calling `get_best_split()`. If it's too small, then we just compute the mean and use that as our node's value."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
