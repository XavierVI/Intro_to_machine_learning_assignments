{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from task1 import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The core idea behind our algorithm is using a binary search tree, where each node has a condition property. When making a prediction, if the condition property is true for some given input $\\vec{x}$, then it will traverse down the right subtree. Otherwise, it will traverse down the left subtree.\n",
    "\n",
    "A few fundamental things to consider are\n",
    "1. How easy will it be to build this tree?\n",
    "2. How likely will the tree be balanced, thus increasing prediction performance?\n",
    "\n",
    "For 1, building the tree will be fairly simple if we use partitioning. We will have to write logic for determining if a node should be a leaf node. We have three conditions under which this will happen.\n",
    "1. The impurity of the dataset for the node is 0.\n",
    "2. The maximum number of leaf nodes have already been created.\n",
    "3. The height of the tree has reached the maximum value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding partitioning\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of using partitioning is to avoid splitting a copy of the entire dataset to each individual node while building the tree.\n",
    "\n",
    "Let $X$ denote our dataset and let $\\vec{x}$ denote a sample in $X$.\n",
    "\n",
    "Assume we've found a sample $\\vec{x}^{(s)} \\in X$ to split the data. This means we want to split the data along at the sample $\\vec{x}^{(s)} along the feature $f$. One way we could do this is to partition the dataset into two separate datasets $X_{l}$ and $X_{r}$.\n",
    "\n",
    "The way we do this is by computing the following. For each sample in $X$, if $\\vec{x}_{f} < \\vec{x}^{(s)}_{f}$ then it goes into $X_{l}$, otherwise, it goes into $X_{r}$. Where this becomes efficient is we can simply store the\n",
    "indices for these samples instead of creating copies of the original dataset. For numpy, the easier way is to create a boolean mask for the rows we want in each partition.\n",
    "\n",
    "The following code is an example of how this works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 6, 14, 18, 17],\n",
       "       [12,  3,  8, 14],\n",
       "       [13, 15, 13,  0],\n",
       "       [ 9,  7, 13,  4]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating an array of random integers\n",
    "X = np.random.default_rng().integers(20, size=(4, 4))\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], shape=(0, 4), dtype=int64)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's say we want to partition the data along the 2nd feature\n",
    "# and the sample we are splitting the data at is the second sample\n",
    "f = 1 # second feature\n",
    "split_value = X[1, f] # second sample (second row)\n",
    "mask = X[:, f] < split_value\n",
    "X_l = X[mask]\n",
    "X_l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determining the best split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To determine the best split, we will simply use a combination of a function that calculates the sum of squares for each possible split of the data and numpy's `argmin()` function.\n",
    "\n",
    "First, we start off with a dataset $X$. Then for each sample $\\vec{x} \\in X$, we split the data into two subsets $X_l$ and $X_r$ using our partitioning method. Then, we compute the sum\n",
    "$$\n",
    "\\sum_{i=1}^{n_{l}}{ (y_{i} - \\bar{y}_{l})^{2} } + \\sum_{j=1}^{n_{r}}{ (y_{j} - \\bar{y}_{r})^{2} }\n",
    "$$\n",
    "and use `argmin()` to get the index of the sample which minimizes this sum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[13 16  0 16]\n",
      " [ 9 10 12  5]\n",
      " [19  1  5  7]\n",
      " [11  8  2  0]]\n",
      "[0.80500292 0.80794079 0.51532556 0.28580138]\n",
      "[[19  1  5  7]\n",
      " [11  8  2  0]]\n",
      "8\n",
      "[[13 16  0 16]]\n"
     ]
    }
   ],
   "source": [
    "# creating a test dataset\n",
    "X = np.random.default_rng(5).integers(20, size=(4, 4))\n",
    "y = np.random.default_rng(5).uniform(size=(4,))\n",
    "print(X)\n",
    "print(y)\n",
    "\n",
    "# split the dataset into two disjoint subsets using X[1, 1] (second feature)\n",
    "left_bool_mask = X[:, 1] < X[1, 1]\n",
    "X_l = X[left_bool_mask]\n",
    "\n",
    "right_bool_mask = X[:, 1] > X[1, 1]\n",
    "X_r = X[right_bool_mask]\n",
    "\n",
    "print(X_l)\n",
    "print(X_l.size)\n",
    "print(X_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2295241809540004"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compute the sum of squares for each split\n",
    "left_sum = X_l.shape[0] * np.std(y[left_bool_mask])\n",
    "right_sum = X_r.shape[0] * np.std(y[right_bool_mask])\n",
    "left_sum + right_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "# generalizing\n",
    "\n",
    "def split_data(X, sample_idx, feature_idx):\n",
    "    left_bool_mask = X[:, feature_idx] < X[sample_idx, feature_idx]\n",
    "    right_bool_mask = X[:, feature_idx] > X[sample_idx, feature_idx]\n",
    "    return left_bool_mask, right_bool_mask\n",
    "\n",
    "def sum_of_squares(y):\n",
    "    return y.shape[0] * np.std(y)\n",
    "\n",
    "def get_best_split(X, y, feature_idx):\n",
    "    left_bool_mask, right_bool_mask = split_data(X, 1, feature_idx)\n",
    "    best_sample_idx = 1\n",
    "    # best sum of squares error\n",
    "    best_sse = sum_of_squares(y[left_bool_mask]) + sum_of_squares(y[right_bool_mask])\n",
    "    for sample_idx in range(0, X.shape[0] - 1):\n",
    "        left_bool_mask, right_bool_mask = split_data(X, sample_idx, feature_idx)\n",
    "        sse = sum_of_squares(y[left_bool_mask]) + \\\n",
    "              sum_of_squares(y[right_bool_mask])\n",
    "        \n",
    "        if sse < best_sse:\n",
    "            best_sample_idx = sample_idx\n",
    "            best_sse = sse\n",
    "    \n",
    "    return best_sample_idx\n",
    "        \n",
    "\n",
    "print(get_best_split(X, y, 1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
